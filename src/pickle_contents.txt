The directory 'src/results' was found. Looking for pickle files...
Contents of flan-t5-base_P_TUNING_1_subject-accs.pickle:
{'public_relations': [[47, 105], 0.44761904761904764], 'elementary_mathematics': [[89, 373], 0.2386058981233244], 'professional_accounting': [[88, 277], 0.3176895306859206], 'electrical_engineering': [[37, 140], 0.2642857142857143], 'high_school_macroeconomics': [[109, 385], 0.2831168831168831], 'moral_scenarios': [[212, 890], 0.23820224719101124], 'college_chemistry': [[30, 95], 0.3157894736842105], 'econometrics': [[28, 109], 0.25688073394495414], 'anatomy': [[35, 130], 0.2692307692307692], 'business_ethics': [[34, 95], 0.35789473684210527], 'human_sexuality': [[39, 126], 0.30952380952380953], 'high_school_microeconomics': [[66, 233], 0.2832618025751073], 'astronomy': [[49, 147], 0.3333333333333333], 'conceptual_physics': [[70, 230], 0.30434782608695654], 'international_law': [[35, 116], 0.3017241379310345], 'professional_medicine': [[67, 267], 0.250936329588015], 'machine_learning': [[31, 107], 0.2897196261682243], 'security_studies': [[71, 240], 0.29583333333333334], 'global_facts': [[29, 95], 0.30526315789473685], 'high_school_mathematics': [[63, 265], 0.23773584905660378], 'prehistory': [[96, 319], 0.30094043887147337], 'high_school_government_and_politics': [[75, 188], 0.39893617021276595], 'formal_logic': [[34, 121], 0.2809917355371901], 'college_computer_science': [[27, 95], 0.28421052631578947], 'high_school_psychology': [[185, 540], 0.3425925925925926], 'professional_psychology': [[144, 607], 0.2372322899505766], 'virology': [[41, 161], 0.2546583850931677], 'logical_fallacies': [[42, 158], 0.26582278481012656], 'high_school_us_history': [[66, 199], 0.3316582914572864], 'high_school_european_history': [[64, 160], 0.4], 'high_school_geography': [[64, 193], 0.3316062176165803], 'high_school_statistics': [[63, 211], 0.2985781990521327], 'college_medicine': [[51, 168], 0.30357142857142855], 'high_school_world_history': [[93, 232], 0.40086206896551724], 'human_aging': [[52, 218], 0.23853211009174313], 'high_school_chemistry': [[48, 198], 0.24242424242424243], 'management': [[37, 98], 0.37755102040816324], 'philosophy': [[72, 306], 0.23529411764705882], 'professional_law': [[405, 1529], 0.2648790058862001], 'us_foreign_policy': [[39, 95], 0.4105263157894737], 'moral_disputes': [[102, 341], 0.2991202346041056], 'world_religions': [[25, 166], 0.15060240963855423], 'jurisprudence': [[34, 103], 0.3300970873786408], 'nutrition': [[100, 301], 0.33222591362126247], 'high_school_computer_science': [[23, 95], 0.24210526315789474], 'clinical_knowledge': [[83, 260], 0.3192307692307692], 'sociology': [[59, 196], 0.3010204081632653], 'high_school_biology': [[89, 305], 0.29180327868852457], 'medical_genetics': [[29, 95], 0.30526315789473685], 'abstract_algebra': [[19, 95], 0.2], 'marketing': [[85, 229], 0.37117903930131], 'college_mathematics': [[21, 95], 0.22105263157894736], 'high_school_physics': [[51, 146], 0.3493150684931507], 'miscellaneous': [[257, 778], 0.33033419023136246], 'college_biology': [[34, 139], 0.2446043165467626], 'college_physics': [[33, 97], 0.3402061855670103], 'computer_security': [[25, 95], 0.2631578947368421]}

Contents of flan-t5-base_LORA_1_subject-accs.pickle:
{'public_relations': [[38, 105], 0.3619047619047619], 'elementary_mathematics': [[89, 373], 0.2386058981233244], 'professional_accounting': [[94, 277], 0.33935018050541516], 'electrical_engineering': [[37, 140], 0.2642857142857143], 'high_school_macroeconomics': [[107, 385], 0.2779220779220779], 'moral_scenarios': [[214, 890], 0.24044943820224718], 'college_chemistry': [[22, 95], 0.23157894736842105], 'econometrics': [[27, 109], 0.24770642201834864], 'anatomy': [[30, 130], 0.23076923076923078], 'business_ethics': [[37, 95], 0.3894736842105263], 'human_sexuality': [[38, 126], 0.30158730158730157], 'high_school_microeconomics': [[72, 233], 0.3090128755364807], 'astronomy': [[38, 147], 0.2585034013605442], 'conceptual_physics': [[58, 230], 0.25217391304347825], 'international_law': [[50, 116], 0.43103448275862066], 'professional_medicine': [[76, 267], 0.2846441947565543], 'machine_learning': [[32, 107], 0.29906542056074764], 'security_studies': [[76, 240], 0.31666666666666665], 'global_facts': [[30, 95], 0.3157894736842105], 'high_school_mathematics': [[63, 265], 0.23773584905660378], 'prehistory': [[96, 319], 0.30094043887147337], 'high_school_government_and_politics': [[60, 188], 0.3191489361702128], 'formal_logic': [[28, 121], 0.23140495867768596], 'college_computer_science': [[22, 95], 0.23157894736842105], 'high_school_psychology': [[176, 540], 0.32592592592592595], 'professional_psychology': [[191, 607], 0.31466227347611203], 'virology': [[46, 161], 0.2857142857142857], 'logical_fallacies': [[52, 158], 0.3291139240506329], 'high_school_us_history': [[79, 199], 0.3969849246231156], 'high_school_european_history': [[64, 160], 0.4], 'high_school_geography': [[60, 193], 0.31088082901554404], 'high_school_statistics': [[56, 211], 0.26540284360189575], 'college_medicine': [[54, 168], 0.32142857142857145], 'high_school_world_history': [[103, 232], 0.44396551724137934], 'human_aging': [[89, 218], 0.40825688073394495], 'high_school_chemistry': [[48, 198], 0.24242424242424243], 'management': [[34, 98], 0.3469387755102041], 'philosophy': [[84, 306], 0.27450980392156865], 'professional_law': [[502, 1529], 0.32831916285153695], 'us_foreign_policy': [[28, 95], 0.29473684210526313], 'moral_disputes': [[117, 341], 0.34310850439882695], 'world_religions': [[44, 166], 0.26506024096385544], 'jurisprudence': [[37, 103], 0.3592233009708738], 'nutrition': [[80, 301], 0.26578073089701], 'high_school_computer_science': [[33, 95], 0.3473684210526316], 'clinical_knowledge': [[89, 260], 0.3423076923076923], 'sociology': [[74, 196], 0.37755102040816324], 'high_school_biology': [[93, 305], 0.30491803278688523], 'medical_genetics': [[34, 95], 0.35789473684210527], 'abstract_algebra': [[19, 95], 0.2], 'marketing': [[116, 229], 0.5065502183406113], 'college_mathematics': [[23, 95], 0.24210526315789474], 'high_school_physics': [[36, 146], 0.2465753424657534], 'miscellaneous': [[269, 778], 0.34575835475578404], 'college_biology': [[35, 139], 0.2517985611510791], 'college_physics': [[21, 97], 0.21649484536082475], 'computer_security': [[34, 95], 0.35789473684210527]}

Contents of flan-t5-base_PROMPT_TUNING_1_subject-accs.pickle:
{'public_relations': [[44, 105], 0.41904761904761906], 'elementary_mathematics': [[86, 373], 0.23056300268096513], 'professional_accounting': [[84, 277], 0.30324909747292417], 'electrical_engineering': [[31, 140], 0.22142857142857142], 'high_school_macroeconomics': [[106, 385], 0.2753246753246753], 'moral_scenarios': [[213, 890], 0.2393258426966292], 'college_chemistry': [[23, 95], 0.24210526315789474], 'econometrics': [[26, 109], 0.23853211009174313], 'anatomy': [[30, 130], 0.23076923076923078], 'business_ethics': [[29, 95], 0.30526315789473685], 'human_sexuality': [[32, 126], 0.25396825396825395], 'high_school_microeconomics': [[61, 233], 0.26180257510729615], 'astronomy': [[36, 147], 0.24489795918367346], 'conceptual_physics': [[68, 230], 0.2956521739130435], 'international_law': [[32, 116], 0.27586206896551724], 'professional_medicine': [[56, 267], 0.20973782771535582], 'machine_learning': [[28, 107], 0.2616822429906542], 'security_studies': [[61, 240], 0.25416666666666665], 'global_facts': [[17, 95], 0.17894736842105263], 'high_school_mathematics': [[61, 265], 0.23018867924528302], 'prehistory': [[90, 319], 0.28213166144200624], 'high_school_government_and_politics': [[77, 188], 0.4095744680851064], 'formal_logic': [[38, 121], 0.3140495867768595], 'college_computer_science': [[24, 95], 0.25263157894736843], 'high_school_psychology': [[177, 540], 0.3277777777777778], 'professional_psychology': [[149, 607], 0.24546952224052718], 'virology': [[38, 161], 0.2360248447204969], 'logical_fallacies': [[42, 158], 0.26582278481012656], 'high_school_us_history': [[67, 199], 0.33668341708542715], 'high_school_european_history': [[58, 160], 0.3625], 'high_school_geography': [[62, 193], 0.32124352331606215], 'high_school_statistics': [[56, 211], 0.26540284360189575], 'college_medicine': [[52, 168], 0.30952380952380953], 'high_school_world_history': [[97, 232], 0.41810344827586204], 'human_aging': [[54, 218], 0.24770642201834864], 'high_school_chemistry': [[45, 198], 0.22727272727272727], 'management': [[36, 98], 0.3673469387755102], 'philosophy': [[77, 306], 0.25163398692810457], 'professional_law': [[407, 1529], 0.26618705035971224], 'us_foreign_policy': [[33, 95], 0.3473684210526316], 'moral_disputes': [[118, 341], 0.3460410557184751], 'world_religions': [[33, 166], 0.19879518072289157], 'jurisprudence': [[29, 103], 0.2815533980582524], 'nutrition': [[97, 301], 0.3222591362126246], 'high_school_computer_science': [[25, 95], 0.2631578947368421], 'clinical_knowledge': [[86, 260], 0.33076923076923076], 'sociology': [[63, 196], 0.32142857142857145], 'high_school_biology': [[82, 305], 0.26885245901639343], 'medical_genetics': [[26, 95], 0.2736842105263158], 'abstract_algebra': [[23, 95], 0.24210526315789474], 'marketing': [[101, 229], 0.4410480349344978], 'college_mathematics': [[24, 95], 0.25263157894736843], 'high_school_physics': [[30, 146], 0.2054794520547945], 'miscellaneous': [[240, 778], 0.30848329048843187], 'college_biology': [[30, 139], 0.2158273381294964], 'college_physics': [[30, 97], 0.30927835051546393], 'computer_security': [[29, 95], 0.30526315789473685]}

Contents of flan-t5-base_PREFIX_TUNING_1_MMLU-acc.pickle:
0.0

Contents of flan-t5-base_DORA_1_subject-accs.pickle:
{'public_relations': [[51, 105], 0.4857142857142857], 'elementary_mathematics': [[84, 373], 0.225201072386059], 'professional_accounting': [[90, 277], 0.3249097472924188], 'electrical_engineering': [[42, 140], 0.3], 'high_school_macroeconomics': [[117, 385], 0.3038961038961039], 'moral_scenarios': [[216, 890], 0.24269662921348314], 'college_chemistry': [[24, 95], 0.25263157894736843], 'econometrics': [[37, 109], 0.3394495412844037], 'anatomy': [[41, 130], 0.3153846153846154], 'business_ethics': [[37, 95], 0.3894736842105263], 'human_sexuality': [[35, 126], 0.2777777777777778], 'high_school_microeconomics': [[75, 233], 0.3218884120171674], 'astronomy': [[44, 147], 0.29931972789115646], 'conceptual_physics': [[65, 230], 0.2826086956521739], 'international_law': [[55, 116], 0.47413793103448276], 'professional_medicine': [[76, 267], 0.2846441947565543], 'machine_learning': [[28, 107], 0.2616822429906542], 'security_studies': [[76, 240], 0.31666666666666665], 'global_facts': [[19, 95], 0.2], 'high_school_mathematics': [[59, 265], 0.22264150943396227], 'prehistory': [[90, 319], 0.28213166144200624], 'high_school_government_and_politics': [[82, 188], 0.43617021276595747], 'formal_logic': [[30, 121], 0.24793388429752067], 'college_computer_science': [[30, 95], 0.3157894736842105], 'high_school_psychology': [[208, 540], 0.3851851851851852], 'professional_psychology': [[187, 607], 0.30807248764415157], 'virology': [[42, 161], 0.2608695652173913], 'logical_fallacies': [[58, 158], 0.3670886075949367], 'high_school_us_history': [[89, 199], 0.4472361809045226], 'high_school_european_history': [[74, 160], 0.4625], 'high_school_geography': [[75, 193], 0.38860103626943004], 'high_school_statistics': [[75, 211], 0.35545023696682465], 'college_medicine': [[64, 168], 0.38095238095238093], 'high_school_world_history': [[105, 232], 0.4525862068965517], 'human_aging': [[70, 218], 0.3211009174311927], 'high_school_chemistry': [[39, 198], 0.19696969696969696], 'management': [[42, 98], 0.42857142857142855], 'philosophy': [[95, 306], 0.3104575163398693], 'professional_law': [[497, 1529], 0.3250490516677567], 'us_foreign_policy': [[38, 95], 0.4], 'moral_disputes': [[116, 341], 0.34017595307917886], 'world_religions': [[41, 166], 0.2469879518072289], 'jurisprudence': [[40, 103], 0.3883495145631068], 'nutrition': [[96, 301], 0.31893687707641194], 'high_school_computer_science': [[27, 95], 0.28421052631578947], 'clinical_knowledge': [[92, 260], 0.35384615384615387], 'sociology': [[79, 196], 0.4030612244897959], 'high_school_biology': [[98, 305], 0.32131147540983607], 'medical_genetics': [[34, 95], 0.35789473684210527], 'abstract_algebra': [[18, 95], 0.18947368421052632], 'marketing': [[120, 229], 0.5240174672489083], 'college_mathematics': [[24, 95], 0.25263157894736843], 'high_school_physics': [[40, 146], 0.273972602739726], 'miscellaneous': [[271, 778], 0.34832904884318766], 'college_biology': [[41, 139], 0.2949640287769784], 'college_physics': [[26, 97], 0.26804123711340205], 'computer_security': [[33, 95], 0.3473684210526316]}

Contents of flan-t5-base_ADALORA_1_subject-accs.pickle:
{'public_relations': [[40, 105], 0.38095238095238093], 'elementary_mathematics': [[85, 373], 0.22788203753351208], 'professional_accounting': [[85, 277], 0.30685920577617326], 'electrical_engineering': [[41, 140], 0.29285714285714287], 'high_school_macroeconomics': [[124, 385], 0.3220779220779221], 'moral_scenarios': [[213, 890], 0.2393258426966292], 'college_chemistry': [[25, 95], 0.2631578947368421], 'econometrics': [[33, 109], 0.30275229357798167], 'anatomy': [[46, 130], 0.35384615384615387], 'business_ethics': [[36, 95], 0.37894736842105264], 'human_sexuality': [[38, 126], 0.30158730158730157], 'high_school_microeconomics': [[77, 233], 0.33047210300429186], 'astronomy': [[46, 147], 0.3129251700680272], 'conceptual_physics': [[65, 230], 0.2826086956521739], 'international_law': [[48, 116], 0.41379310344827586], 'professional_medicine': [[87, 267], 0.3258426966292135], 'machine_learning': [[25, 107], 0.2336448598130841], 'security_studies': [[84, 240], 0.35], 'global_facts': [[19, 95], 0.2], 'high_school_mathematics': [[63, 265], 0.23773584905660378], 'prehistory': [[98, 319], 0.3072100313479624], 'high_school_government_and_politics': [[86, 188], 0.4574468085106383], 'formal_logic': [[29, 121], 0.2396694214876033], 'college_computer_science': [[25, 95], 0.2631578947368421], 'high_school_psychology': [[219, 540], 0.40555555555555556], 'professional_psychology': [[189, 607], 0.3113673805601318], 'virology': [[45, 161], 0.2795031055900621], 'logical_fallacies': [[62, 158], 0.3924050632911392], 'high_school_us_history': [[76, 199], 0.38190954773869346], 'high_school_european_history': [[68, 160], 0.425], 'high_school_geography': [[83, 193], 0.43005181347150256], 'high_school_statistics': [[81, 211], 0.38388625592417064], 'college_medicine': [[62, 168], 0.36904761904761907], 'high_school_world_history': [[98, 232], 0.4224137931034483], 'human_aging': [[75, 218], 0.3440366972477064], 'high_school_chemistry': [[51, 198], 0.25757575757575757], 'management': [[42, 98], 0.42857142857142855], 'philosophy': [[96, 306], 0.3137254901960784], 'professional_law': [[490, 1529], 0.32047089601046436], 'us_foreign_policy': [[37, 95], 0.3894736842105263], 'moral_disputes': [[141, 341], 0.41348973607038125], 'world_religions': [[34, 166], 0.20481927710843373], 'jurisprudence': [[30, 103], 0.2912621359223301], 'nutrition': [[91, 301], 0.3023255813953488], 'high_school_computer_science': [[28, 95], 0.29473684210526313], 'clinical_knowledge': [[97, 260], 0.3730769230769231], 'sociology': [[83, 196], 0.42346938775510207], 'high_school_biology': [[96, 305], 0.31475409836065577], 'medical_genetics': [[32, 95], 0.3368421052631579], 'abstract_algebra': [[20, 95], 0.21052631578947367], 'marketing': [[120, 229], 0.5240174672489083], 'college_mathematics': [[24, 95], 0.25263157894736843], 'high_school_physics': [[40, 146], 0.273972602739726], 'miscellaneous': [[270, 778], 0.34704370179948585], 'college_biology': [[42, 139], 0.302158273381295], 'college_physics': [[30, 97], 0.30927835051546393], 'computer_security': [[29, 95], 0.30526315789473685]}

Contents of flan-t5-base_PROMPT_TUNING_1_MMLU-acc.pickle:
0.0

Contents of flan-t5-base_IA3_1_MMLU-acc.pickle:
0.0

Contents of flan-t5-base_DORA_1_MMLU-acc.pickle:
0.0

Contents of flan-t5-base_PREFIX_TUNING_1_subject-accs.pickle:
{'public_relations': [[43, 105], 0.4095238095238095], 'elementary_mathematics': [[95, 373], 0.2546916890080429], 'professional_accounting': [[84, 277], 0.30324909747292417], 'electrical_engineering': [[45, 140], 0.32142857142857145], 'high_school_macroeconomics': [[119, 385], 0.3090909090909091], 'moral_scenarios': [[216, 890], 0.24269662921348314], 'college_chemistry': [[20, 95], 0.21052631578947367], 'econometrics': [[29, 109], 0.26605504587155965], 'anatomy': [[38, 130], 0.2923076923076923], 'business_ethics': [[42, 95], 0.4421052631578947], 'human_sexuality': [[43, 126], 0.3412698412698413], 'high_school_microeconomics': [[77, 233], 0.33047210300429186], 'astronomy': [[48, 147], 0.32653061224489793], 'conceptual_physics': [[65, 230], 0.2826086956521739], 'international_law': [[56, 116], 0.4827586206896552], 'professional_medicine': [[83, 267], 0.31086142322097376], 'machine_learning': [[39, 107], 0.3644859813084112], 'security_studies': [[88, 240], 0.36666666666666664], 'global_facts': [[19, 95], 0.2], 'high_school_mathematics': [[65, 265], 0.24528301886792453], 'prehistory': [[102, 319], 0.31974921630094044], 'high_school_government_and_politics': [[84, 188], 0.44680851063829785], 'formal_logic': [[35, 121], 0.2892561983471074], 'college_computer_science': [[27, 95], 0.28421052631578947], 'high_school_psychology': [[221, 540], 0.40925925925925927], 'professional_psychology': [[197, 607], 0.3245469522240527], 'virology': [[50, 161], 0.3105590062111801], 'logical_fallacies': [[59, 158], 0.37341772151898733], 'high_school_us_history': [[81, 199], 0.40703517587939697], 'high_school_european_history': [[76, 160], 0.475], 'high_school_geography': [[86, 193], 0.44559585492227977], 'high_school_statistics': [[69, 211], 0.32701421800947866], 'college_medicine': [[59, 168], 0.35119047619047616], 'high_school_world_history': [[104, 232], 0.4482758620689655], 'human_aging': [[81, 218], 0.37155963302752293], 'high_school_chemistry': [[45, 198], 0.22727272727272727], 'management': [[46, 98], 0.46938775510204084], 'philosophy': [[102, 306], 0.3333333333333333], 'professional_law': [[501, 1529], 0.3276651406147809], 'us_foreign_policy': [[43, 95], 0.45263157894736844], 'moral_disputes': [[144, 341], 0.4222873900293255], 'world_religions': [[44, 166], 0.26506024096385544], 'jurisprudence': [[38, 103], 0.36893203883495146], 'nutrition': [[101, 301], 0.33554817275747506], 'high_school_computer_science': [[29, 95], 0.30526315789473685], 'clinical_knowledge': [[97, 260], 0.3730769230769231], 'sociology': [[83, 196], 0.42346938775510207], 'high_school_biology': [[82, 305], 0.26885245901639343], 'medical_genetics': [[35, 95], 0.3684210526315789], 'abstract_algebra': [[23, 95], 0.24210526315789474], 'marketing': [[125, 229], 0.5458515283842795], 'college_mathematics': [[19, 95], 0.2], 'high_school_physics': [[40, 146], 0.273972602739726], 'miscellaneous': [[293, 778], 0.37660668380462725], 'college_biology': [[39, 139], 0.2805755395683453], 'college_physics': [[24, 97], 0.24742268041237114], 'computer_security': [[36, 95], 0.37894736842105264]}

Contents of flan-t5-base_P_TUNING_1_MMLU-acc.pickle:
0.0

Contents of flan-t5-base_IA3_1_subject-accs.pickle:
{'public_relations': [[46, 105], 0.4380952380952381], 'elementary_mathematics': [[87, 373], 0.23324396782841822], 'professional_accounting': [[86, 277], 0.3104693140794224], 'electrical_engineering': [[46, 140], 0.32857142857142857], 'high_school_macroeconomics': [[121, 385], 0.3142857142857143], 'moral_scenarios': [[226, 890], 0.2539325842696629], 'college_chemistry': [[24, 95], 0.25263157894736843], 'econometrics': [[33, 109], 0.30275229357798167], 'anatomy': [[45, 130], 0.34615384615384615], 'business_ethics': [[39, 95], 0.4105263157894737], 'human_sexuality': [[45, 126], 0.35714285714285715], 'high_school_microeconomics': [[80, 233], 0.34334763948497854], 'astronomy': [[53, 147], 0.36054421768707484], 'conceptual_physics': [[64, 230], 0.2782608695652174], 'international_law': [[55, 116], 0.47413793103448276], 'professional_medicine': [[90, 267], 0.33707865168539325], 'machine_learning': [[24, 107], 0.22429906542056074], 'security_studies': [[96, 240], 0.4], 'global_facts': [[22, 95], 0.23157894736842105], 'high_school_mathematics': [[57, 265], 0.21509433962264152], 'prehistory': [[106, 319], 0.3322884012539185], 'high_school_government_and_politics': [[83, 188], 0.44148936170212766], 'formal_logic': [[36, 121], 0.2975206611570248], 'college_computer_science': [[27, 95], 0.28421052631578947], 'high_school_psychology': [[220, 540], 0.4074074074074074], 'professional_psychology': [[203, 607], 0.3344316309719934], 'virology': [[50, 161], 0.3105590062111801], 'logical_fallacies': [[61, 158], 0.3860759493670886], 'high_school_us_history': [[84, 199], 0.4221105527638191], 'high_school_european_history': [[75, 160], 0.46875], 'high_school_geography': [[82, 193], 0.42487046632124353], 'high_school_statistics': [[74, 211], 0.35071090047393366], 'college_medicine': [[60, 168], 0.35714285714285715], 'high_school_world_history': [[105, 232], 0.4525862068965517], 'human_aging': [[79, 218], 0.3623853211009174], 'high_school_chemistry': [[51, 198], 0.25757575757575757], 'management': [[44, 98], 0.4489795918367347], 'philosophy': [[102, 306], 0.3333333333333333], 'professional_law': [[496, 1529], 0.3243950294310007], 'us_foreign_policy': [[43, 95], 0.45263157894736844], 'moral_disputes': [[147, 341], 0.4310850439882698], 'world_religions': [[39, 166], 0.23493975903614459], 'jurisprudence': [[41, 103], 0.39805825242718446], 'nutrition': [[104, 301], 0.34551495016611294], 'high_school_computer_science': [[28, 95], 0.29473684210526313], 'clinical_knowledge': [[99, 260], 0.38076923076923075], 'sociology': [[88, 196], 0.4489795918367347], 'high_school_biology': [[94, 305], 0.3081967213114754], 'medical_genetics': [[34, 95], 0.35789473684210527], 'abstract_algebra': [[23, 95], 0.24210526315789474], 'marketing': [[126, 229], 0.5502183406113537], 'college_mathematics': [[22, 95], 0.23157894736842105], 'high_school_physics': [[38, 146], 0.2602739726027397], 'miscellaneous': [[292, 778], 0.37532133676092544], 'college_biology': [[41, 139], 0.2949640287769784], 'college_physics': [[31, 97], 0.31958762886597936], 'computer_security': [[34, 95], 0.35789473684210527]}

Contents of flan-t5-base_ADALORA_1_MMLU-acc.pickle:
0.0

Contents of flan-t5-base_LORA_1_MMLU-acc.pickle:
0.0

